{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch, torch.nn as nn, torch.nn.functional as F\nimport math","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-13T16:18:38.038620Z","iopub.execute_input":"2024-07-13T16:18:38.039418Z","iopub.status.idle":"2024-07-13T16:18:42.017493Z","shell.execute_reply.started":"2024-07-13T16:18:38.039356Z","shell.execute_reply":"2024-07-13T16:18:42.015976Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:18:42.019800Z","iopub.execute_input":"2024-07-13T16:18:42.020580Z","iopub.status.idle":"2024-07-13T16:18:42.087658Z","shell.execute_reply.started":"2024-07-13T16:18:42.020543Z","shell.execute_reply":"2024-07-13T16:18:42.086337Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"class sLSTMCell(nn.Module):\n    def __init__(self, input_size, hidden_size, device):\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.device = device\n\n        self.weight_ih = nn.Parameter(torch.randn(4 * hidden_size, input_size, device=device))\n        self.weight_hh = nn.Parameter(torch.randn(4 * hidden_size, hidden_size, device=device))\n        self.bias      = nn.Parameter(torch.randn(4 * hidden_size, device=device))\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.weight_ih)\n        nn.init.xavier_uniform_(self.weight_hh)\n        nn.init.zeros_(self.bias)\n\n    def forward(self, input, hx):\n        h, c, n, m = hx\n        gates = input @ self.weight_ih.T + h @ self.weight_hh.T + self.bias\n\n        z_tilde, i_tilde, f_tilde, o_tilde = gates.chunk(4, 1)\n\n        z = torch.tanh(z_tilde)\n        i = torch.exp(i_tilde)\n        f = torch.exp(f_tilde)\n        o = torch.sigmoid(o_tilde)\n\n        m_t = torch.maximum(torch.log(f) + m, torch.log(i))\n        i_prime = torch.exp(torch.log(i) - m_t)\n        f_prime = torch.exp(torch.log(f) + m - m_t)\n\n        c = f_prime * c + i_prime * z\n        n = f_prime * n + i_prime\n        h_tilde = c / n\n        h = o * h_tilde\n\n        return h, c, n, m_t","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:18:44.761596Z","iopub.execute_input":"2024-07-13T16:18:44.761937Z","iopub.status.idle":"2024-07-13T16:18:44.775098Z","shell.execute_reply.started":"2024-07-13T16:18:44.761913Z","shell.execute_reply":"2024-07-13T16:18:44.773842Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class sLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, dropout=0.0, device=\"cpu\"):\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.device = device\n        self.layers = nn.ModuleList([\n            sLSTMCell(input_size if i == 0 else hidden_size, hidden_size, device)\n            for i in range(num_layers)\n        ])\n        self.dropout_layer = nn.Dropout(dropout)\n\n    def forward(self, input, hidden_state=None):\n        bs, seq_len, _ = input.size()\n        if hidden_state is None:\n            hidden_state = [(\n                torch.zeros(bs, self.hidden_size, device=self.device),\n                torch.zeros(bs, self.hidden_size, device=self.device),\n                torch.ones (bs, self.hidden_size, device=self.device),\n                torch.zeros(bs, self.hidden_size, device=self.device)\n            ) for _ in range(self.num_layers)]\n\n        outputs = []\n        for t in range(seq_len):\n            x = input[:, t, :]\n            for layer_idx, layer in enumerate(self.layers):\n                h, c, n, m = hidden_state[layer_idx]\n                h, c, n, m = layer(x, (h, c, n, m))\n                hidden_state[layer_idx] = (h, c, n, m)\n                x = self.dropout_layer(h) if layer_idx < self.num_layers - 1 else h\n            outputs.append(x)\n\n        return torch.stack(outputs, dim=1), hidden_state","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:18:45.685760Z","iopub.execute_input":"2024-07-13T16:18:45.686187Z","iopub.status.idle":"2024-07-13T16:18:45.697762Z","shell.execute_reply.started":"2024-07-13T16:18:45.686158Z","shell.execute_reply":"2024-07-13T16:18:45.696814Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"x = torch.randn(1, 10, 64).to(device)\nmodel = sLSTM(64, 128, 2, device=device)\noutput, states = model(x)\nprint(output.size())","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:18:45.922699Z","iopub.execute_input":"2024-07-13T16:18:45.923061Z","iopub.status.idle":"2024-07-13T16:18:46.450829Z","shell.execute_reply.started":"2024-07-13T16:18:45.923031Z","shell.execute_reply":"2024-07-13T16:18:46.449780Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"torch.Size([1, 10, 128])\n","output_type":"stream"}]},{"cell_type":"code","source":"class mLSTMCell(nn.Module):\n    def __init__(self, input_size, hidden_size, device=\"cpu\"):\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.device = device\n\n        # Input, forget, and output gates\n        self.w_i = nn.Parameter(torch.randn(hidden_size, input_size, device=device))\n        self.w_f = nn.Parameter(torch.randn(hidden_size, input_size, device=device))\n        self.w_o = nn.Parameter(torch.randn(hidden_size, input_size, device=device))\n        self.b_i = nn.Parameter(torch.zeros(hidden_size, device=device))\n        self.b_f = nn.Parameter(torch.zeros(hidden_size, device=device))\n        self.b_o = nn.Parameter(torch.zeros(hidden_size, device=device))\n        \n        self.w_q = nn.Linear(input_size, hidden_size, device=device)\n        self.w_k = nn.Linear(input_size, hidden_size, device=device)\n        self.w_v = nn.Linear(input_size, hidden_size, device=device)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.w_i)\n        nn.init.xavier_uniform_(self.w_f)\n        nn.init.xavier_uniform_(self.w_o)\n        nn.init.zeros_(self.b_i)\n        nn.init.zeros_(self.b_f)\n        nn.init.zeros_(self.b_o)\n        nn.init.xavier_uniform_(self.w_q.weight)\n        nn.init.xavier_uniform_(self.w_k.weight)\n        nn.init.xavier_uniform_(self.w_v.weight)\n        nn.init.zeros_(self.w_q.bias)\n        nn.init.zeros_(self.w_k.bias)\n        nn.init.zeros_(self.w_v.bias)\n\n    def forward(self, input, hx):\n        h, c, n = hx\n        \n        # compute gates\n        i_t = torch.exp(input @ self.w_i.T + self.b_i) # input_gate\n        f_t = torch.sigmoid(input @ self.w_f.T + self.b_f) # forget_gate\n        o_t = torch.sigmoid(input @ self.w_o.T + self.b_o) # output_gate\n        \n        q_t = self.w_q(input) # query\n        k_t = self.w_k(input) / math.sqrt(self.hidden_size) # key\n        v_t = self.w_v(input) # value\n        \n        # update cell state and normalizer state\n        c = f_t * c + i_t * (v_t * k_t) # cell_state\n        n = f_t * n + i_t * k_t # normalizer_state\n        \n        # compute hidden state\n        h_tilde = c * q_t\n        denom = torch.clamp(torch.abs(n * q_t), min=1.0)\n        h = o_t * (h_tilde / denom)\n\n        return h, c, n","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:18:48.734477Z","iopub.execute_input":"2024-07-13T16:18:48.735189Z","iopub.status.idle":"2024-07-13T16:18:48.751829Z","shell.execute_reply.started":"2024-07-13T16:18:48.735154Z","shell.execute_reply":"2024-07-13T16:18:48.750570Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class mLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, dropout=0.0, device=\"cpu\"):\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.device = device\n        self.layers = nn.ModuleList([\n            mLSTMCell(input_size if i == 0 else hidden_size, hidden_size, device=device)\n            for i in range(num_layers)\n        ])\n        self.dropout_layer = nn.Dropout(dropout)\n\n    def forward(self, input, hidden_state=None):\n        bs, seq_len, _ = input.size()\n        if hidden_state is None:\n            hidden_state = [(\n                torch.zeros(bs, self.hidden_size, device=self.device),\n                torch.zeros(bs, self.hidden_size, device=self.device),\n                torch.zeros(bs, self.hidden_size, device=self.device)\n            ) for _ in range(self.num_layers)]\n\n        outputs = []\n        for t in range(seq_len):\n            x = input[:, t, :]\n            for layer_idx, layer in enumerate(self.layers):\n                h, c, n = hidden_state[layer_idx]\n                h, c, n = layer(x, (h, c, n))\n                hidden_state[layer_idx] = (h, c, n)\n                x = self.dropout_layer(h) if layer_idx < self.num_layers - 1 else h\n            outputs.append(x)\n\n        return torch.stack(outputs, dim=1), hidden_state","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:18:48.963803Z","iopub.execute_input":"2024-07-13T16:18:48.964200Z","iopub.status.idle":"2024-07-13T16:18:48.981098Z","shell.execute_reply.started":"2024-07-13T16:18:48.964166Z","shell.execute_reply":"2024-07-13T16:18:48.979741Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"x = torch.randn(1, 10, 64).to(device)\nmodel = mLSTM(64, 128, 2, device=device)\noutput, states = model(x)\nprint(output.size())","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:18:49.824630Z","iopub.execute_input":"2024-07-13T16:18:49.825077Z","iopub.status.idle":"2024-07-13T16:18:49.885126Z","shell.execute_reply.started":"2024-07-13T16:18:49.825041Z","shell.execute_reply":"2024-07-13T16:18:49.884138Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"torch.Size([1, 10, 128])\n","output_type":"stream"}]},{"cell_type":"code","source":"class xLSTMBlock(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, dropout=0.0, lstm_type=\"slstm\", device=\"cpu\"):\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.lstm_type = lstm_type\n        self.device = device\n\n        if self.lstm_type == \"slstm\":\n            self.lstm = sLSTM(input_size, hidden_size, num_layers, dropout, device=device)\n        if self.lstm_type == \"mlstm\":\n            self.lstm = mLSTM(input_size, hidden_size, num_layers, dropout, device=device)\n\n        self.norm = nn.LayerNorm(hidden_size)\n        self.act = nn.GELU()\n        self.dropout_layer = nn.Dropout(dropout)\n        self.proj = nn.Linear(hidden_size, input_size)\n\n    def forward(self, input, hidden_state=None):\n        lstm_output, hidden_state = self.lstm(input, hidden_state)\n        output = self.act(lstm_output)\n        output = self.norm(output)\n        output = self.proj(output)\n        output = self.dropout_layer(output + input)\n        return output, hidden_state","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:18:51.390874Z","iopub.execute_input":"2024-07-13T16:18:51.391901Z","iopub.status.idle":"2024-07-13T16:18:51.400950Z","shell.execute_reply.started":"2024-07-13T16:18:51.391864Z","shell.execute_reply":"2024-07-13T16:18:51.399910Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class xLSTM(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_size, num_layers, num_blocks, dropout=0.0, lstm_type=\"slstm\", device=device):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.num_blocks = num_blocks\n        self.dropout = dropout\n        self.lstm_type = lstm_type\n\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.blocks = nn.ModuleList([xLSTMBlock(embed_dim, hidden_size, num_layers, dropout, lstm_type, device=device) for _ in range(self.num_blocks)])\n        self.output_layer = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, input, hidden_state=None):\n        embed_seq = self.embedding(input)\n        if hidden_state is None:\n            hidden_state = [None] * self.num_blocks\n        output_seq = embed_seq\n        for i, block in enumerate(self.blocks):\n            output_seq, hidden_state[i] = block(output_seq, hidden_state[i])\n        output_seq = self.output_layer(output_seq)\n        return output_seq, hidden_state","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:18:51.626343Z","iopub.execute_input":"2024-07-13T16:18:51.626745Z","iopub.status.idle":"2024-07-13T16:18:51.636342Z","shell.execute_reply.started":"2024-07-13T16:18:51.626719Z","shell.execute_reply":"2024-07-13T16:18:51.635241Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"vocab_size = 1000\nembed_dim = 128\nhidden_size = 64\nnum_layers = 2\nnum_blocks = 3\ndropout = 0.1\nlstm_type = \"mlstm\"\n\nmodel = xLSTM(vocab_size, embed_dim, hidden_size, num_layers, num_blocks, dropout, lstm_type, device=device)\nmodel.to(device)\n\nbs = 4\nseq_len = 32\ninput_data = torch.randint(0, vocab_size, (bs, seq_len)).to(device)\n\noutput, hidden_state = model(input_data)\n\nprint(f\"Input shape: {input_data.shape}\")\nprint(f\"Output shape: {output.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:18:53.087575Z","iopub.execute_input":"2024-07-13T16:18:53.087914Z","iopub.status.idle":"2024-07-13T16:18:53.394528Z","shell.execute_reply.started":"2024-07-13T16:18:53.087889Z","shell.execute_reply":"2024-07-13T16:18:53.393560Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Input shape: torch.Size([4, 32])\nOutput shape: torch.Size([4, 32, 1000])\n","output_type":"stream"}]},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:18:54.033173Z","iopub.execute_input":"2024-07-13T16:18:54.034092Z","iopub.status.idle":"2024-07-13T16:18:55.337098Z","shell.execute_reply.started":"2024-07-13T16:18:54.034058Z","shell.execute_reply":"2024-07-13T16:18:55.335990Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"--2024-07-13 16:18:55--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: 'input.txt'\n\ninput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n\n2024-07-13 16:18:55 (34.3 MB/s) - 'input.txt' saved [1115394/1115394]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\nlen(text)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:18:55.339513Z","iopub.execute_input":"2024-07-13T16:18:55.340238Z","iopub.status.idle":"2024-07-13T16:18:55.349432Z","shell.execute_reply.started":"2024-07-13T16:18:55.340200Z","shell.execute_reply":"2024-07-13T16:18:55.348425Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"1115394"},"metadata":{}}]},{"cell_type":"code","source":"# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.1*len(data)) # first 90% will be train, rest val\ntrain_data = data[n:]\nval_data = data[:n]\n\ndef get_batch(split):\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x.to(device), y.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:18:56.473113Z","iopub.execute_input":"2024-07-13T16:18:56.473509Z","iopub.status.idle":"2024-07-13T16:18:56.776013Z","shell.execute_reply.started":"2024-07-13T16:18:56.473479Z","shell.execute_reply":"2024-07-13T16:18:56.775247Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"block_size = 100  # sequence length\nbatch_size = 64\nembed_dim = 128\nhidden_size = 256\nnum_layers = 2\nnum_blocks = 3\ndropout = 0.1\nlstm_type = \"mlstm\"\nlearning_rate = 0.001\nnum_epochs = 10\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = xLSTM(vocab_size, embed_dim, hidden_size, num_layers, num_blocks, dropout, lstm_type, device)\nmodel.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:19:07.609894Z","iopub.execute_input":"2024-07-13T16:19:07.610290Z","iopub.status.idle":"2024-07-13T16:19:07.633524Z","shell.execute_reply.started":"2024-07-13T16:19:07.610257Z","shell.execute_reply":"2024-07-13T16:19:07.632406Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:19:07.835784Z","iopub.execute_input":"2024-07-13T16:19:07.836498Z","iopub.status.idle":"2024-07-13T16:19:07.840652Z","shell.execute_reply.started":"2024-07-13T16:19:07.836462Z","shell.execute_reply":"2024-07-13T16:19:07.839627Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def train(model, criterion, optimizer, num_epochs):\n    model.train()\n    for epoch in range(num_epochs):\n        total_loss = 0\n        num_batches = len(train_data) // (block_size * batch_size)\n        progress_bar = tqdm(range(num_batches), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n\n        for _ in progress_bar:\n            batch_input, batch_target = get_batch('train')\n\n            optimizer.zero_grad()\n            output, _ = model(batch_input)\n            loss = criterion(output.view(-1, vocab_size), batch_target.view(-1))\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n\n            total_loss += loss.item()\n            progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n\n        avg_loss = total_loss / num_batches\n        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n\n        # Validation\n        model.eval()\n        with torch.no_grad():\n            val_loss = 0\n            num_val_batches = len(val_data) // (block_size * batch_size)\n            for _ in range(num_val_batches):\n                batch_input, batch_target = get_batch('val')\n                output, _ = model(batch_input)\n                val_loss += criterion(output.view(-1, vocab_size), batch_target.view(-1)).item()\n            avg_val_loss = val_loss / num_val_batches\n            print(f\"Validation Loss: {avg_val_loss:.4f}\")\n        model.train()\n\n    print(\"Training completed.\")","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:19:08.187081Z","iopub.execute_input":"2024-07-13T16:19:08.187783Z","iopub.status.idle":"2024-07-13T16:19:08.198173Z","shell.execute_reply.started":"2024-07-13T16:19:08.187729Z","shell.execute_reply":"2024-07-13T16:19:08.197240Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"train(model, criterion, optimizer, num_epochs)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:19:09.227540Z","iopub.execute_input":"2024-07-13T16:19:09.227900Z","iopub.status.idle":"2024-07-13T16:46:29.028824Z","shell.execute_reply.started":"2024-07-13T16:19:09.227871Z","shell.execute_reply":"2024-07-13T16:46:29.027744Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 156/156 [02:42<00:00,  1.04s/it, Loss=1.7723]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Average Loss: 2.2751\nValidation Loss: 1.7929\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 156/156 [02:39<00:00,  1.02s/it, Loss=1.6055]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/10, Average Loss: 1.7135\nValidation Loss: 1.6471\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 156/156 [02:39<00:00,  1.02s/it, Loss=1.5220]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/10, Average Loss: 1.5914\nValidation Loss: 1.5653\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 156/156 [02:38<00:00,  1.01s/it, Loss=1.5310]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/10, Average Loss: 1.5296\nValidation Loss: 1.5283\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 156/156 [02:38<00:00,  1.02s/it, Loss=1.5003]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/10, Average Loss: 1.4842\nValidation Loss: 1.4907\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 156/156 [02:38<00:00,  1.02s/it, Loss=1.4458]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/10, Average Loss: 1.4552\nValidation Loss: 1.4766\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 156/156 [02:38<00:00,  1.01s/it, Loss=1.3985]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/10, Average Loss: 1.4279\nValidation Loss: 1.4599\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 156/156 [02:38<00:00,  1.02s/it, Loss=1.4377]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/10, Average Loss: 1.4090\nValidation Loss: 1.4504\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 156/156 [02:40<00:00,  1.03s/it, Loss=1.3824]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/10, Average Loss: 1.3921\nValidation Loss: 1.4349\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 156/156 [02:40<00:00,  1.03s/it, Loss=1.3683]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/10, Average Loss: 1.3780\nValidation Loss: 1.4366\nTraining completed.\n","output_type":"stream"}]},{"cell_type":"code","source":"def generate_text(model, start_text, length=200, temperature=1.0):\n    model.eval()\n    context = torch.tensor(encode(start_text), dtype=torch.long).unsqueeze(0).to(device)\n    generated_text = start_text\n\n    with torch.no_grad():\n        for _ in range(length):\n            output, _ = model(context)\n            probs = (output[0, -1] / temperature).softmax(dim=-1)\n            next_char_idx = torch.multinomial(probs, num_samples=1).item()\n            generated_text += itos[next_char_idx]\n            context = torch.cat((context, torch.tensor([[next_char_idx]], device=device)), dim=1)\n            if context.size(1) > block_size:\n                context = context[:, -block_size:]\n\n    return generated_text","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:46:29.038295Z","iopub.execute_input":"2024-07-13T16:46:29.038698Z","iopub.status.idle":"2024-07-13T16:46:29.053008Z","shell.execute_reply.started":"2024-07-13T16:46:29.038663Z","shell.execute_reply":"2024-07-13T16:46:29.051627Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"sample_text = generate_text(model, start_text=\"The \", length=1024, temperature=0.7)\nprint(\"Generated Text:\")\nprint(sample_text)","metadata":{"execution":{"iopub.status.busy":"2024-07-13T16:46:29.054526Z","iopub.execute_input":"2024-07-13T16:46:29.054975Z","iopub.status.idle":"2024-07-13T16:50:46.705237Z","shell.execute_reply.started":"2024-07-13T16:46:29.054939Z","shell.execute_reply":"2024-07-13T16:50:46.704083Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Generated Text:\nThe guilty shall not see the fond in your commites\nNot to vile his friar Romeo standing four pleasure.\n\nKING RICHARD II:\nI take her be being like a fear,\nAnd she was be were many profess are to child.\n\nFirst Servant:\nWhat, lords; the noble begin from the true!\n\nKING RICHARD III:\nFarewell.\nFor mercy means that no man's head\nIt to discourse the post little law on thy master,\nThou excepts to Lalong to bear the very serve to be thou wilt thou shalt e'er we weep.\nIt come, when thou hast to a death,\nThat what can there we say the king as thou art to see.\n\nLUCENTIO:\nWhy, what I must be it, not seemed with him.\n\nDUKE OF YORK:\nA widow, I cannot make thy beauty.\n\nMENENIUS:\nHow took you, my lord; and so I but the duke now,\nAnd bless the world meet to keep the maid,\nAnd you would to be a, which we are with Paris:\nThe cares must come to any more jewel?\n\nKING RICHARD III:\nWhy, I have not so, you may the many heaven.\n\nJULIET:\nWhat, when the order sound sin crown of the earth,\nNone hath made him friends, like a shame, stand\nWhic\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}