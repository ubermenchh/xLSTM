{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "n4LgOIZyUj2A"
      },
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WeLebHzxwZdn",
        "outputId": "4505619d-18e0-4bba-b61c-af37605eb19a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class sLSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, device):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.device = device\n",
        "\n",
        "        self.weight_ih = nn.Parameter(torch.randn(4 * hidden_size, input_size, device=device))\n",
        "        self.weight_hh = nn.Parameter(torch.randn(4 * hidden_size, hidden_size, device=device))\n",
        "        self.bias      = nn.Parameter(torch.randn(4 * hidden_size, device=device))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.weight_ih)\n",
        "        nn.init.xavier_uniform_(self.weight_hh)\n",
        "        nn.init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, input, hx):\n",
        "        h, c, n, m = hx\n",
        "        gates = input @ self.weight_ih.T + h @ self.weight_hh.T + self.bias\n",
        "\n",
        "        z_tilde, i_tilde, f_tilde, o_tilde = gates.chunk(4, 1)\n",
        "\n",
        "        z = torch.tanh(z_tilde)\n",
        "        i = torch.exp(i_tilde)\n",
        "        f = torch.exp(f_tilde)\n",
        "        o = torch.sigmoid(o_tilde)\n",
        "\n",
        "        m_t = torch.maximum(torch.log(f) + m, torch.log(i))\n",
        "        i_prime = torch.exp(torch.log(i) - m_t)\n",
        "        f_prime = torch.exp(torch.log(f) + m - m_t)\n",
        "\n",
        "        c = f_prime * c + i_prime * z\n",
        "        n = f_prime * n + i_prime\n",
        "        h_tilde = c / n\n",
        "        h = o * h_tilde\n",
        "\n",
        "        return h, c, n, m_t"
      ],
      "metadata": {
        "id": "2f1lFF4zUqeZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class sLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout=0.0, device=\"cpu\"):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.device = device\n",
        "        self.layers = nn.ModuleList([\n",
        "            sLSTMCell(input_size if i == 0 else hidden_size, hidden_size, device)\n",
        "            for i in range(num_layers)\n",
        "        ])\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden_state=None):\n",
        "        bs, seq_len, _ = input.size()\n",
        "        if hidden_state is None:\n",
        "            hidden_state = [(\n",
        "                torch.zeros(bs, self.hidden_size, device=self.device),\n",
        "                torch.zeros(bs, self.hidden_size, device=self.device),\n",
        "                torch.ones (bs, self.hidden_size, device=self.device),\n",
        "                torch.zeros(bs, self.hidden_size, device=self.device)\n",
        "            ) for _ in range(self.num_layers)]\n",
        "\n",
        "        outputs = []\n",
        "        for t in range(seq_len):\n",
        "            x = input[:, t, :]\n",
        "            for layer_idx, layer in enumerate(self.layers):\n",
        "                h, c, n, m = hidden_state[layer_idx]\n",
        "                h, c, n, m = layer(x, (h, c, n, m))\n",
        "                hidden_state[layer_idx] = (h, c, n, m)\n",
        "                x = self.dropout_layer(h) if layer_idx < self.num_layers - 1 else h\n",
        "            outputs.append(x)\n",
        "\n",
        "        return torch.stack(outputs, dim=1), hidden_state"
      ],
      "metadata": {
        "id": "55acKNMhW7Bt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1, 10, 64).to(device)\n",
        "model = sLSTM(64, 128, 2, device=device)\n",
        "output, states = model(x)\n",
        "print(output.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UlFiy3PYQgj",
        "outputId": "4721ab5d-3d36-4bf1-f435-7ab2ab434f9e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class mLSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, device=\"cpu\"):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.device = device\n",
        "\n",
        "        self.weight_ih = nn.Parameter(torch.randn(3 * hidden_size, input_size, device=device))\n",
        "        self.weight_hh = nn.Parameter(torch.randn(3 * hidden_size, hidden_size, device=device))\n",
        "        self.bias = nn.Parameter(torch.randn(3 * hidden_size, device=device))\n",
        "\n",
        "        self.w_q = nn.Linear(input_size, hidden_size, device=device)\n",
        "        self.w_k = nn.Linear(input_size, hidden_size, device=device)\n",
        "        self.w_v = nn.Linear(input_size, hidden_size, device=device)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.weight_ih)\n",
        "        nn.init.xavier_uniform_(self.weight_hh)\n",
        "        nn.init.zeros_(self.bias)\n",
        "        nn.init.xavier_uniform_(self.w_q.weight)\n",
        "        nn.init.xavier_uniform_(self.w_k.weight)\n",
        "        nn.init.xavier_uniform_(self.w_v.weight)\n",
        "        nn.init.zeros_(self.w_q.bias)\n",
        "        nn.init.zeros_(self.w_k.bias)\n",
        "        nn.init.zeros_(self.w_v.bias)\n",
        "\n",
        "    def forward(self, input, hx):\n",
        "        h, c = hx\n",
        "        gates = input @ self.weight_ih.T + h @ self.weight_hh.T + self.bias\n",
        "\n",
        "        i, f, o = gates.chunk(3, 1)\n",
        "\n",
        "        i = torch.exp(i) # input gate\n",
        "        f = torch.exp(f) # forget gate\n",
        "        o = torch.sigmoid(o) # output gate\n",
        "\n",
        "        q = self.w_q(input) # query\n",
        "        k = self.w_k(input) # key\n",
        "        v = self.w_v(input) # value\n",
        "\n",
        "        c = f.unsqueeze(2) * c + i.unsqueeze(2) * torch.bmm(v.unsqueeze(2), k.unsqueeze(1)) # cell state\n",
        "        h = o * torch.bmm(q.unsqueeze(1), c).squeeze(1) # hidden state\n",
        "\n",
        "        return h, c"
      ],
      "metadata": {
        "id": "l6WX6AijYaj9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class mLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout=0.0, device=\"cpu\"):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.device = device\n",
        "        self.layers = nn.ModuleList([\n",
        "            mLSTMCell(input_size if i == 0 else hidden_size, hidden_size, device=device)\n",
        "            for i in range(num_layers)\n",
        "        ])\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden_state=None):\n",
        "        bs, seq_len, _ = input.size()\n",
        "        if hidden_state is None:\n",
        "            hidden_state = [(\n",
        "                torch.zeros(bs, self.hidden_size, device=self.device),\n",
        "                torch.zeros(bs, self.hidden_size, device=self.device)\n",
        "            ) for _ in range(self.num_layers)]\n",
        "\n",
        "        outputs = []\n",
        "        for t in range(seq_len):\n",
        "            x = input[:, t, :]\n",
        "            for layer_idx, layer in enumerate(self.layers):\n",
        "                h, c = hidden_state[layer_idx]\n",
        "                h, c = layer(x, (h, c))\n",
        "                hidden_state[layer_idx] = (h, c)\n",
        "                x = self.dropout_layer(h) if layer_idx < self.num_layers - 1 else h\n",
        "            outputs.append(x)\n",
        "\n",
        "        return torch.stack(outputs, dim=1), hidden_state"
      ],
      "metadata": {
        "id": "dflANTR5McMH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1, 10, 64).to(device)\n",
        "model = mLSTM(64, 128, 2, device=device)\n",
        "output, states = model(x)\n",
        "print(output.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzZHsZJWL42h",
        "outputId": "8b404fcc-9639-41ca-a0bb-cafa867e12da"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class xLSTMBlock(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout=0.0, lstm_type=\"slstm\", device=\"cpu\"):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.lstm_type = lstm_type\n",
        "        self.device = device\n",
        "\n",
        "        if self.lstm_type == \"slstm\":\n",
        "            self.lstm = sLSTM(input_size, hidden_size, num_layers, dropout, device=device)\n",
        "        if self.lstm_type == \"mlstm\":\n",
        "            self.lstm = mLSTM(input_size, hidden_size, num_layers, dropout, device=device)\n",
        "\n",
        "        self.norm = nn.LayerNorm(hidden_size)\n",
        "        self.act = nn.GELU()\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "        self.proj = nn.Linear(hidden_size, input_size)\n",
        "\n",
        "    def forward(self, input, hidden_state=None):\n",
        "        lstm_output, hidden_state = self.lstm(input, hidden_state)\n",
        "        output = self.act(lstm_output)\n",
        "        output = self.norm(output)\n",
        "        output = self.proj(output)\n",
        "        output = self.dropout_layer(output + input)\n",
        "        return output, hidden_state"
      ],
      "metadata": {
        "id": "tivPMokhMY7u"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class xLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_size, num_layers, num_blocks, dropout=0.0, lstm_type=\"slstm\", device=device):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.num_blocks = num_blocks\n",
        "        self.dropout = dropout\n",
        "        self.lstm_type = lstm_type\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.blocks = nn.ModuleList([xLSTMBlock(embed_dim, hidden_size, num_layers, dropout, lstm_type, device=device) for _ in range(self.num_blocks)])\n",
        "        self.output_layer = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input, hidden_state=None):\n",
        "        embed_seq = self.embedding(input)\n",
        "        if hidden_state is None:\n",
        "            hidden_state = [None] * self.num_blocks\n",
        "        output_seq = embed_seq\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            output_seq, hidden_state[i] = block(output_seq, hidden_state[i])\n",
        "        output_seq = self.output_layer(output_seq)\n",
        "        return output_seq, hidden_state"
      ],
      "metadata": {
        "id": "p5uBu4_ZgOUy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 1000\n",
        "embed_dim = 128\n",
        "hidden_size = 64\n",
        "num_layers = 2\n",
        "num_blocks = 3\n",
        "dropout = 0.1\n",
        "lstm_type = \"slstm\"\n",
        "\n",
        "model = xLSTM(vocab_size, embed_dim, hidden_size, num_layers, num_blocks, dropout, lstm_type, device=device)\n",
        "model.to(device)\n",
        "\n",
        "bs = 4\n",
        "seq_len = 32\n",
        "input_data = torch.randint(0, vocab_size, (bs, seq_len)).to(device)\n",
        "\n",
        "output, hidden_state = model(input_data)\n",
        "\n",
        "print(f\"Input shape: {input_data.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1qpP3e6iKv7",
        "outputId": "b13d5515-3813-43bb-9f61-9c7c0b72c6dd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([4, 32])\n",
            "Output shape: torch.Size([4, 32, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3R2NjAtldeZ",
        "outputId": "5c266f08-dd9b-4bf6-be45-3146e243d8d9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-12 18:24:41--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-07-12 18:24:41 (46.0 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaknYP_Frc5P",
        "outputId": "c8f98a67-da63-4474-85ca-4333add006c8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.1*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[n:]\n",
        "val_data = data[:n]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x.to(device), y.to(device)"
      ],
      "metadata": {
        "id": "76UuiurUstr0"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 100  # sequence length\n",
        "batch_size = 64\n",
        "embed_dim = 128\n",
        "hidden_size = 256\n",
        "num_layers = 2\n",
        "num_blocks = 3\n",
        "dropout = 0.1\n",
        "lstm_type = \"slstm\"\n",
        "learning_rate = 0.001\n",
        "num_epochs = 5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = xLSTM(vocab_size, embed_dim, hidden_size, num_layers, num_blocks, dropout, lstm_type, device)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "94T1NsbwSZmw"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "UtnV2ex1Slxw"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        num_batches = len(train_data) // (block_size * batch_size)\n",
        "        progress_bar = tqdm(range(num_batches), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for _ in progress_bar:\n",
        "            batch_input, batch_target = get_batch('train')\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output, _ = model(batch_input)\n",
        "            loss = criterion(output.view(-1, vocab_size), batch_target.view(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss = 0\n",
        "            num_val_batches = len(val_data) // (block_size * batch_size)\n",
        "            for _ in range(num_val_batches):\n",
        "                batch_input, batch_target = get_batch('val')\n",
        "                output, _ = model(batch_input)\n",
        "                val_loss += criterion(output.view(-1, vocab_size), batch_target.view(-1)).item()\n",
        "            avg_val_loss = val_loss / num_val_batches\n",
        "            print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "        model.train()\n",
        "\n",
        "    print(\"Training completed.\")"
      ],
      "metadata": {
        "id": "fPXtmz6-4o9q"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfqE5idgSppl",
        "outputId": "f62002b5-a9ab-4cd1-cc15-0f30c6d3b71c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 156/156 [14:41<00:00,  5.65s/it, Loss=1.8946]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Average Loss: 2.3960\n",
            "Validation Loss: 1.9230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 156/156 [15:41<00:00,  6.04s/it, Loss=1.6215]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Average Loss: 1.7498\n",
            "Validation Loss: 1.6623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 156/156 [16:33<00:00,  6.37s/it, Loss=1.5156]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Average Loss: 1.5843\n",
            "Validation Loss: 1.5606\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 156/156 [16:34<00:00,  6.38s/it, Loss=1.4924]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Average Loss: 1.5008\n",
            "Validation Loss: 1.5116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 156/156 [16:47<00:00,  6.46s/it, Loss=1.4191]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Average Loss: 1.4434\n",
            "Validation Loss: 1.4659\n",
            "Training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_text, length=200, temperature=1.0):\n",
        "    model.eval()\n",
        "    context = torch.tensor(encode(start_text), dtype=torch.long).unsqueeze(0).to(device)\n",
        "    generated_text = start_text\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(length):\n",
        "            output, _ = model(context)\n",
        "            probs = (output[0, -1] / temperature).softmax(dim=-1)\n",
        "            next_char_idx = torch.multinomial(probs, num_samples=1).item()\n",
        "            generated_text += itos[next_char_idx]\n",
        "            context = torch.cat((context, torch.tensor([[next_char_idx]], device=device)), dim=1)\n",
        "            if context.size(1) > block_size:\n",
        "                context = context[:, -block_size:]\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "X-XDQZA3Ssnh"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = generate_text(model, start_text=\"The \", length=1024, temperature=0.7)\n",
        "print(\"Generated Text:\")\n",
        "print(sample_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDXRxNomTD0R",
        "outputId": "228f82d7-1224-4dd4-f04b-297e06bf3bf2"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "The departed true heavens did so fair last.\n",
            "\n",
            "LUCIO:\n",
            "How dost thou more straight not perform this body with his\n",
            "happy brother of the ready of the ears make me.\n",
            "\n",
            "GLOUCESTER:\n",
            "Where is that I is a man the deep.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "He had it who can shall hear me but a guest,\n",
            "How you shall accomple to thee with my son.\n",
            "\n",
            "SLY:\n",
            "You shall be more of mine of time and so wit.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "Now, which end your dease is heart of care.\n",
            "\n",
            "POLIXENES:\n",
            "The prince and scares of a steal and calls\n",
            "And thou wilt with me so proparion of a bornes we\n",
            "Anople that your hungnot of my sit\n",
            "What is the bounds conspainted of men.\n",
            "\n",
            "CATESBY:\n",
            "Good marriage with stir what may be death,\n",
            "For come not see my heart be, thou wilt the father will\n",
            "The preorer hand, my lord, if a general comes\n",
            "That even my more king's hundly part,\n",
            "And where those lady think it were that I,\n",
            "And before the jest have peace interching.\n",
            "\n",
            "KING RICHARD III:\n",
            "Or, therefore false father, fair so nothing;\n",
            "For I may know her should not have a day, lords\n",
            "You shall be from his safely ha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3nd9uhOSTHvy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}